{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532c53cc-d025-464a-99c3-b44dfd1fc3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (24.0)\n"
     ]
    }
   ],
   "source": [
    "!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daefed87-855a-481f-98cd-133aa2a0dba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.21.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from selenium) (0.25.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: chromedriver_autoinstaller in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.4)\n",
      "Requirement already satisfied: packaging>=23.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromedriver_autoinstaller) (24.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install chromedriver_autoinstaller\n",
    "!pip install pandas\n",
    "!pip install openpyxl\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097f9f67-cb7f-4dfe-aa7a-9d28339bd97c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo==3.11 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pymongo[srv]==3.11) (3.11.0)\n",
      "Requirement already satisfied: dnspython<2.0.0,>=1.16.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pymongo[srv]==3.11) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install \"pymongo[srv]\"==3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d4998c-2535-472b-aee7-961b3ff68df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from pymongo import MongoClient\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import os\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "from PIL import Image as PILImage\n",
    "import requests\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b60103-2dcd-4c9f-820d-e0c1f5e10852",
   "metadata": {},
   "source": [
    "## 예스24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df2e280a-bff5-4cff-86f9-d092227b80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yes24(keyword):\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    for num in range(5):\n",
    "        url = f\"https://www.yes24.com/Product/Search?domain=ALL&query={keyword}&page={num+1}\"\n",
    "        driver.get(url)\n",
    "\n",
    "        for i in range(20):\n",
    "            driver.execute_script(f'window.scrollTo(0, document.body.scrollHeight / 20 * {i})')\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        li_elements_xpath = '/html/body/div[1]/div[4]/div/div[2]/section[2]/div[3]/ul/li'\n",
    "        li_elements = driver.find_elements(By.XPATH, li_elements_xpath)\n",
    "        \n",
    "        for li in li_elements:\n",
    "            try:\n",
    "                img = li.find_element(By.TAG_NAME, 'img')\n",
    "                title = li.find_element(By.CLASS_NAME,'gd_name')\n",
    "                author = li.find_element(By.CLASS_NAME, 'info_auth').text.split(' 저')[0]\n",
    "                publish = li.find_element(By.CLASS_NAME, 'info_pub').text\n",
    "                created = li.find_element(By.CLASS_NAME, 'info_date').text\n",
    "                price = li.find_element(By.CLASS_NAME, 'yes_b').text\n",
    "                site = 'http://www.yes24.com/Main/default.aspx'\n",
    "                \n",
    "                title = img.get_attribute('alt')\n",
    "                img_src = img.get_attribute('src')\n",
    "                \n",
    "                reset()\n",
    "                img_list.append(img_src)\n",
    "                title_list.append(title)\n",
    "                author_list.append(author)\n",
    "                publish_list.append(publish)\n",
    "                created_list.append(created)\n",
    "                price_list.append(price)\n",
    "                site_list[0] = site\n",
    "                \n",
    "                \n",
    "            except:\n",
    "                print('오류')\n",
    "                continue  # 이미지가 없는 경우 건너뜀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae409503-6d59-4839-ba58-2d6e614f0da5",
   "metadata": {},
   "source": [
    "## 알라딘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af056f40-e7f7-4b69-a7b1-e4869e83fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aladin(keyword):\n",
    "    driver = webdriver.Chrome()\n",
    "    url = f'https://www.aladin.co.kr/m/msearch.aspx?SearchWord={keyword}&SearchTarget=All'\n",
    "    driver.get(url)\n",
    "\n",
    "    for num in range(5):        \n",
    "        pages = driver.find_element(By.XPATH, '/html/body/div[13]/div/ul')\n",
    "        a_link = pages.find_elements(By.TAG_NAME, 'a')\n",
    "        page = a_link[num].click()\n",
    "    \n",
    "        for i in range(20):\n",
    "            driver.execute_script(f'window.scrollTo(0, document.body.scrollHeight / 20 * {i})')\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        li_elements_xpath = '/html/body/div[12]'\n",
    "        li_elements = driver.find_elements(By.CLASS_NAME, 'browse_list_box')\n",
    "        \n",
    "        for li in li_elements:\n",
    "            try:\n",
    "                img = li.find_elements(By.CLASS_NAME, 'front_cover')\n",
    "                if img:\n",
    "                    img_src = img[0].get_attribute('src')\n",
    "                else:\n",
    "                    img_src = li.find_element(By.CLASS_NAME, 'i_cover').get_attribute('src')\n",
    "                title = li.find_element(By.CLASS_NAME,'b_book_t').text.split('] ')[1]\n",
    "                author = li.find_elements(By.TAG_NAME, 'li')[1].text.split(' (지은이)')[0]\n",
    "                elements = li.find_elements(By.TAG_NAME, 'li')[2].text\n",
    "                publish = elements.split(' | ')[0]\n",
    "                created = elements.split(' | ')[1]\n",
    "                price = li.find_element(By.CLASS_NAME, 'b_price').text\n",
    "                site = 'https://www.aladin.co.kr/home/welcome.aspx'\n",
    "    \n",
    "                reset()\n",
    "                img_list.append(img_src)\n",
    "                title_list.append(title)\n",
    "                author_list.append(author)\n",
    "                publish_list.append(publish)\n",
    "                created_list.append(created)\n",
    "                price_list.append(price)\n",
    "                site_list[0] = site\n",
    "            except:\n",
    "                print('오류')\n",
    "                continue  # 이미지가 없는 경우 건너뜀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e2a37-badf-4d27-afa8-f6920286c4f5",
   "metadata": {},
   "source": [
    "## 교보문구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba2613b-4958-4560-adec-86aace484fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kyobo(keyword):\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    for num in range(5):\n",
    "        url = f\"https://search.kyobobook.co.kr/search?keyword={keyword}&gbCode=TOT&page={num+1}\"\n",
    "        driver.get(url)\n",
    "    \n",
    "        for i in range(20):\n",
    "            driver.execute_script(f\"window.scrollTo(0, 1000 * {i})\")\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        li_element_xpath = \"/html/body/div[3]/main/section/div/div/div[4]/div[2]/div/div[2]/div[3]/ul\"\n",
    "        li_element_xpath_load = driver.find_element(By.XPATH, li_element_xpath)\n",
    "        li_elements = li_element_xpath_load.find_elements(By.CLASS_NAME, \"prod_item\")\n",
    "    \n",
    "    \n",
    "        for li in li_elements:\n",
    "            try:\n",
    "                img = li.find_element(By.CLASS_NAME, \"prod_img_load\")\n",
    "                title = li.find_element(By.CLASS_NAME, \"prod_info\").text.split('] ')[-1]\n",
    "                author = li.find_element(By.CLASS_NAME, \"author.rep\").text\n",
    "                publish = li.find_element(By.CLASS_NAME, \"prod_publish\").text.split(\"\\n\")[0]\n",
    "                created = li.find_element(By.CLASS_NAME, \"prod_publish\").text.split(\"\\n\")[1]\n",
    "                try:\n",
    "                    price = li.find_element(By.CLASS_NAME, \"price\").text\n",
    "                except:\n",
    "                    price = li.find_element(By.CLASS_NAME, \"prod_purchase_state\").text\n",
    "                img_src = img.get_attribute(\"src\")\n",
    "                site = 'http://www.kyobobook.co.kr/'\n",
    "    \n",
    "                reset()\n",
    "                img_list.append(img_src)\n",
    "                title_list.append(title)\n",
    "                author_list.append(author)\n",
    "                publish_list.append(publish)\n",
    "                created_list.append(created)\n",
    "                price_list.append(price)\n",
    "                site_list[0] = site\n",
    "            except:\n",
    "                print(\"오류\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "473af759-de8d-4c14-8f9b-696ccbd3e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전역변수들을 빈 리스트로 리셋\n",
    "def reset():\n",
    "    collection = ''\n",
    "    img_list = []\n",
    "    title_list = []\n",
    "    author_list = []\n",
    "    publish_list = []\n",
    "    created_list = []\n",
    "    price_list = []\n",
    "    site_list = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b034306-e8b3-4cc1-b327-fe0bb0e971a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이트 지정하여 검색어를 입력받아 크롤링한 데이터를 몽고DB에 저장\n",
    "def search(num):\n",
    "    url = 'mongodb+srv://mnbv7952:hXHFetjGCo06jkTn@cluster0.marjbqu.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0'\n",
    "    client = MongoClient(url)\n",
    "    database = client['exam4']\n",
    "    word = input(\"검색어 입력: \")\n",
    "\t\t\t\t\t\t\n",
    "    if num =='1':\n",
    "        collection = database['yes24']\n",
    "        yes24(word)\n",
    "    elif num=='2':\n",
    "        collection = database['aladin']\n",
    "        aladin(word)\n",
    "    elif num == '3':\n",
    "        collection = database['kyobo']\n",
    "        kyobo(word)\n",
    "\n",
    "    for i in range(len(img_list)):\n",
    "        books_insert.append({\n",
    "            '이미지로컬경로':img_list[i],\n",
    "            '책제목':title_list[i],\n",
    "            '저자':author_list[i],\n",
    "            '출판사':publish_list[i],\n",
    "            '출판일':created_list[i],\n",
    "            '가격':price_list[i],\n",
    "            '판매사이트':site_list[0]\n",
    "        })\n",
    "    # collection.insert_many(books_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "011655bb-c8b2-44e4-b48c-e3bc52363693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 생성 및, 각 서점 시트 생성\n",
    "def create_excel():\n",
    "    if not os.path.exists('books.xlsx'):\n",
    "        wb = openpyxl.Workbook()\n",
    "        \n",
    "        default_sheet = wb.active\n",
    "        default_sheet.title = \"yes24\"\n",
    "        \n",
    "        sheet2 = wb.create_sheet(title=\"aladin\")\n",
    "        sheet3 = wb.create_sheet(title=\"kyobo\")\n",
    "        \n",
    "        wb.save(\"books.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "232442e2-e328-4f2c-ac5c-5c469abc1351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 시트에 책 데이터 저장.\n",
    "def append_excel(sheet_name):\n",
    "    wb = openpyxl.load_workbook('books.xlsx')\n",
    "    sheet = wb[sheet_name]\n",
    "    \n",
    "    sheet['A1'] = '책제목'\n",
    "    sheet['B1'] = '저자'\n",
    "    sheet['C1'] = '가격'\n",
    "    sheet['D1'] = '출판사'\n",
    "    sheet['E1'] = '출판일'\n",
    "    sheet['F1'] = '판매사이트'\n",
    "\n",
    "    # 셀 너비 지정\n",
    "    for col in string.ascii_uppercase:\n",
    "        sheet.column_dimensions[col].width = 15\n",
    "    \n",
    "    # 이미지는 더 넓게 설정\n",
    "    sheet.column_dimensions['F'].width = 25\n",
    "    sheet.column_dimensions['G'].width = 25\n",
    "\n",
    "    for i in range(len(img_list)):\n",
    "        img_src = img_list[i]\n",
    "        title =title_list[i]\n",
    "        author =author_list[i]\n",
    "        publish =publish_list[i]\n",
    "        created =created_list[i]\n",
    "        price =price_list[i]\n",
    "        site = site_list[0]\n",
    "        if sheet_name =='yes24':\n",
    "            img_src = img_src.split('/')[-2]\n",
    "        else:\n",
    "            img_src = img_src.split('/')[-1]\n",
    "        img_src = f'./{sheet_name}/{img_src}'\n",
    "        sheet.row_dimensions[i+2].height = 200 # 셀 높이 지정\n",
    "\n",
    "        # 각 사이트별 폴더에 있는 이미지를 엑셀이미지로 변환 후, 삽입\n",
    "        img = PILImage.open(img_src)\n",
    "        img = ExcelImage(img)\n",
    "        img.anchor = f'G{i+2}'\n",
    "        sheet.add_image(img)\n",
    "    \n",
    "        sheet.append([title, author, price, publish, created, site])\n",
    "    \n",
    "    wb.save('books.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24cf216b-4238-42ea-95f6-8b4aa9997381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 사이트 폴더 생성 후, 이미지 파일 저장\n",
    "def image_storage(site):\n",
    "    if not os.path.exists(site):\n",
    "        os.makedirs(site)\n",
    "\n",
    "    for img in img_list:\n",
    "        response = requests.get(img)\n",
    "        if site =='yes24':\n",
    "            name = img.split('/')[-2]\n",
    "        else:\n",
    "            name = img.split('/')[-1]\n",
    "        with open(os.path.join(site, name), 'wb') as f:\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "242b2030-c57f-410f-9695-af94924c262f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.yes24 | 2.aladin | 3.kyobo\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "사이트 선택(숫자입력):  3\n",
      "검색어 입력:  파이썬\n"
     ]
    }
   ],
   "source": [
    "print('1.yes24 | 2.aladin | 3.kyobo')\n",
    "num = input(\"사이트 선택(숫자입력): \")\n",
    "# 어떤 사이트에서 크롤링할건지 입력받음\n",
    "\n",
    "books_insert = []\n",
    "collection = ''\n",
    "img_list = []\n",
    "title_list = []\n",
    "author_list = []\n",
    "publish_list = []\n",
    "created_list = []\n",
    "price_list = []\n",
    "site_list = ['']\n",
    "sheet = ''\n",
    "\n",
    "search(num) # 해당 사이트에서 크롤링 함수 호출\n",
    "create_excel() # 'books'이라는 엑셀파일 생성.(있으면 생성 안됌)\n",
    "if num=='1':\n",
    "    sheet = 'yes24'\n",
    "elif num=='2':\n",
    "    sheet = 'aladin'\n",
    "elif num=='3':\n",
    "    sheet = 'kyobo'\n",
    "image_storage(sheet) # 해당 사이트명으로 이미지 폴더 생성하여 이미지 저장\n",
    "append_excel(sheet) # 엑셀에 이미지 및 데이터 넣기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b89abb-7772-4c5f-8244-206759bc01c6",
   "metadata": {},
   "source": [
    "### 실행 방법(jupyter notebook으로 작성함.)\n",
    "- 위에서부터 차례대로 실행한다.\n",
    "- 마지막 셀에서 실행 시,\n",
    "- 크롤링 할 사이트(숫자로만) 입력 -> 검색어 입력 하여\n",
    "- 크롤링 및 이미지저장 + 엑셀파일에 저장"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
